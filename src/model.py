"""OpenAI API handler for generating text for the README.md file."""

import asyncio
import os
from typing import Dict, Tuple

import httpx
import openai
import spacy
from cachetools import TTLCache

from logger import Logger
from utils import reformat_sentence

ENGINE = "text-davinci-003"
ENDPOINT = f"https://api.openai.com/v1/engines/{ENGINE}/completions"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MAX_TOKENS = 4096

NLP = spacy.load("en_core_web_sm")
LOGGER = Logger("readmeai_logger")

# Add TTLCache with maximum size of 500 items and 600 seconds of TTL.
cache = TTLCache(maxsize=500, ttl=600)

http_client = httpx.AsyncClient(
    http2=True,
    timeout=30,
    limits=httpx.Limits(max_keepalive_connections=10, max_connections=100),
)


class OpenAIError(Exception):
    """
    Custom exception class for OpenAI errors.

    Attributes
    ----------
    Exception : str
        The error message for the OpenAI API error.
    """


async def code_to_text(
    ignore_files: list, files: Dict[str, str], prompt: str
) -> Dict[str, str]:
    """
    Generate summary text for code files using OpenAI's GPT-3.

    Parameters
    ----------
    ignore_files : list
        A list of file names to ignore when generating summaries.
    files : Dict[str, str]
        A dictionary where the keys are file paths and values are raw code.
    prompt : str
        The prompt to send to OpenAI's GPT API.
    Returns
    -------
    Dict[str, str]
       Processed summary text for each file.
    """

    tasks = []

    for path, contents in files.items():
        if any(fn in str(path) for fn in ignore_files):
            LOGGER.debug(f"Skipping file: {path}")
            continue

        LOGGER.info(f"Davinci processing: {path}")

        prompt_code = prompt.format(contents)
        prompt_length = len(prompt_code.split())

        if prompt_length > MAX_TOKENS:
            err = "Prompt exceeds max token limit: {}"
            tasks.append(
                asyncio.create_task(null_summary(path, err.format(prompt_length)))
            )
            LOGGER.debug(err.format(prompt_code))
            continue

        # Use async client with connection pooling
        tasks.append(asyncio.create_task(fetch_summary(path, prompt_code)))

    results = await asyncio.gather(*tasks)

    return results


async def fetch_summary(file: str, prompt: str) -> Tuple[str, str]:
    """
    Fetch summary text for a given file path using OpenAI's GPT-3 API.

    Parameters
    ----------
    file : str
        The file path for which to fetch summary text.
    prompt : str
        The prompt to send to OpenAI's GPT-3 API.

    Returns
    -------
    Tuple[str, str]
        A tuple containing the file path and the generated summary text.
    """

    # Use cache if the same prompt has already been fetched
    if prompt in cache:
        return (file, cache[prompt])

    response = await http_client.post(
        ENDPOINT,
        json={
            "prompt": prompt,
            "temperature": 0,
            "max_tokens": 69,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
        },
        headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
    )

    if response.status_code != 200:
        LOGGER.error(f"Error fetching summary for {file}: {response.text}")
        return (file, "Error generating file summary.")

    response.raise_for_status()
    data = response.json()

    if "choices" not in data or len(data["choices"]) == 0:
        raise OpenAIError("OpenAI response missing 'choices' field.")

    file_summary = data["choices"][0]["text"]

    summary = spacy_text_processor(file_summary)
    summary = reformat_sentence(summary)

    # Add file summary to cache
    cache[prompt] = summary

    return (file, summary)


def generate_summary_text(prompt: str) -> str:
    """
    Prompts the OpenAI large language model API to
    generate summaries for each file in the repository.

    Parameters
    ----------
    prompt : str
        The prompt to send to OpenAI's GPT API.

    Returns
    -------
    str
        Text generated by OpenAI's GPT API.
    """

    completions = openai.Completion.create(
        engine=ENGINE,
        prompt=prompt,
        max_tokens=69,
    )
    generated_text = completions.choices[0].text
    return generated_text.lstrip().strip('"')


async def null_summary(file: str, summary: str) -> Tuple[str, str]:
    """
    Provides a placeholder summary tuple for scenarios when the
    the OpenAI API fails to generate a summary from the prompt.

    Parameters
    ----------
    file : str
        The file path for which to create a null summary.
    summary : str
        The null summary to be returned.

    Returns
    -------
    Tuple[str, str]
        A tuple containing the file path and the null summary.
    """

    return (file, summary)


def spacy_text_processor(text: str) -> str:
    """
    Process a text string using Spacy's NLP pipeline.

    Parameters
    ----------
    text : str
        The text to process.

    Returns
    -------
    str
        The processed text.
    """
    doc = NLP(text)
    processed_text = " ".join([token.text for token in doc])
    return processed_text
